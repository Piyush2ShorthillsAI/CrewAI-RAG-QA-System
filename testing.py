from sentence_transformers import SentenceTransformer, util
import json
from llm2 import load_llm
from query_to_pinecone_final import query
from llm2 import query_llm   # Ensure Pinecone is configured properly
from fuzzywuzzy import fuzz
import json

# Load the pre-trained similarity model
similarity_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

def is_similar(new_answer, expected_answer, threshold=0.5):
    """
    Check if new_answer is semantically or textually similar to the expected_answer
    using FuzzyWuzzy for fuzzy matching and SentenceTransformers for meaning-based similarity.
    
    Parameters:
    - new_answer (str): The new answer generated by the model.
    - expected_answer (str): The expected answer to compare against.
    - threshold (float): The threshold (0-1) to consider similarity as True (default: 0.2).
    
    Returns:
    - bool: True if the answers are meaning-wise or textually similar, False otherwise.
    """
    
    # Use FuzzyWuzzy for fuzzy matching (textual similarity)
    fuzz_score = fuzz.ratio(new_answer.lower(), expected_answer.lower()) / 100  # Convert to a scale of 0-1
    
    # If fuzzy match score is above threshold, return True
    if fuzz_score >= threshold:
        return True
    
    # If FuzzyWuzzy doesn't match, we use SentenceTransformer for semantic similarity
    new_answer_embedding = similarity_model.encode(new_answer, convert_to_tensor=True)
    expected_answer_embedding = similarity_model.encode(expected_answer, convert_to_tensor=True)
    
    # Compute cosine similarity between the two answers
    similarity_score = util.pytorch_cos_sim(new_answer_embedding, expected_answer_embedding)
    
    # If semantic similarity score is above threshold, consider them similar
    return similarity_score[0][0] > threshold


def load_test_cases(test_cases_file):
    """
    Load the test cases from the specified JSON file.
    
    Parameters:
    - test_cases_file (str): The path to the test cases JSON file.
    
    Returns:
    - list: A list of test cases with 'question' and 'answer' fields.
    """
    try:
        with open(test_cases_file, "r", encoding="utf-8") as file:
            test_cases = json.load(file)
        return test_cases
    except Exception as e:
        print(f"Error loading test cases: {e}")
        return []


def validate_model_with_test_cases(test_cases, llm_pipeline):
    """
    Validates the model using the provided test cases and compares the LLM response with expected answers.
    """
    passed = 21
    failed = 15
    filename = "test_results.log"
    with open(filename, "w") as file:
      for test_case in test_cases:
        question = test_case["question"]
        expected_answer = test_case["answer"]
        
        # Get the context (optional - you can adjust this based on your setup)
        context =   query(question)  # You can provide context based on your setup
        # print(f"Context: {context}")
        print(f"\nQuerying model for question{passed + failed+1}: '{question}'")

        # Get the model's response
        model_response = query_llm(context,question, llm_pipeline)
        
        # Check the similarity between the expected answer and the model's response
        if is_similar(model_response, expected_answer):
            passed += 1
        else:
            failed += 1
           
        similarity_percentage = (passed/(passed + failed)) * 100 if (passed + failed) > 0 else 0
            
        log_entry = {
                "query_number": passed + failed,
                "question": question,
                "expected_answer": expected_answer,
                "model_response": model_response,
                f"similarity_percentage_query_1_to_{passed + failed}": similarity_percentage,
                "passed": passed,
                "failed": failed
            }
            
        file.write(json.dumps(log_entry) + "\n")
        file.flush()
            
        print(f"Query {passed+failed}: Similarity Percentage: {similarity_percentage}%")    
        
    print(f"Test results logged to {filename}")
    
def main():
    # Path to the test cases file (generated_test_cases.json)
    TEST_CASES_FILE = "generated_test_cases.json"  # Updated to use the generated_test_cases.json file
    
    # Load test cases
    test_cases = load_test_cases(TEST_CASES_FILE)

    if test_cases:
        # Initialize the LLM pipeline (you should replace this with your actual LLM loading function)
        llm_pipeline = load_llm()  # Replace this with your LLM pipeline loading function

        # Validate the model using the test cases
        validate_model_with_test_cases(test_cases, llm_pipeline)

if __name__ == "__main__":
    main()
